---
layout:       post
title:        "支持向量机"
subtitle:     "—— Support Vector Machine"
date:         2017-12-29 23:00:00
author:       "Jelliy"
header-img:   "img/port-bg-starry sky.jpg"
header-mask:  0.3
catalog:      true
multilingual: false
tags:
    - 深度学习
---

## 基本概念
SVM - Support Vector Machine。支持向量机，其含义是通过支持向量运算的分类器。其中“机”的意思是机器，可以理解为分类器。
什么是支持向量呢？在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。
见下图，在一个二维环境中，其中点R，S，G点和其它靠近中间黑线的点可以看作为支持向量，它们可以决定分类器，也就是黑线的具体参数。

![](/img/in-post/SVM/1.png)

* 分类器：就是分类函数。
* 线性分类：可以理解为在2维空间中，可以通过一条直线来分类。在p维空间中，可以通过一个p-1维的超平面来分类。
* 向量：有多个属性的变量。在多维空间中的一个点就是一个向量。比如 \\(x = (x_1, x_2, ..., x_n)\\)。下面的ww也是向量。
* 约束条件(subject to) ： 在求一个函数的最优值时需要满足的约束条件。
* 向量相乘: \\(w^{T}x = \textstyle \sum_{i=1}^n w_ix_i\\)
* 内积: \\(\langle x,y \rangle = \textstyle \sum_{i=1}^n x_iy_i\\)

## 核心思想

* SVM的目的是要找到一个线性分类的最佳超平面 \\(f(x)=w^{T}x+b=0\\)。求 w 和 b。
* 首先通过两个分类的最近点，找到f(x)的约束条件。
* 有了约束条件，就可以通过拉格朗日乘子法和KKT条件来求解，这时，问题变成了求拉格朗日乘子\\(\alpha_i\\)和 b。
* 对于异常点的情况，加入松弛变量ξ来处理。
* 使用SMO来求拉格朗日乘子\\(\alpha_i\\)和b。这时，我们会发现有些\\(\alpha_i=0\\)，这些点就可以不用在分类器中考虑了。
* 惊喜! 不用求w了，可以使用拉格朗日乘子\\(\alpha_i\\)和b作为分类器的参数。
* 非线性分类的问题：映射到高维度、使用核函数。

## 目标函数

具体而言，如果你考察这样一个数据集，其中有正样本，也有负样本，可以看到这个数
据集是线性可分的。我的意思是，存在一条直线把正负样本分开。当然有多条不同的直线，
可以把正样本和负样本完全分开。

比如，这就是一个决策边界可以把正样本和负样本分开。但是多多少少这个看起来并不
是非常自然是么?
或者我们可以画一条更差的决策界，这是另一条决策边界，可以将正样本和负样本分开，
但仅仅是勉强分开，这些决策边界看起来都不是特别好的选择，支持向量机将会选择这个黑
色的决策边界，相较于之前我用粉色或者绿色画的决策界。这条黑色的看起来好得多，黑线
看起来是更稳健的决策界。在分离正样本和负样本上它显得的更好。数学上来讲，这是什么
意思呢？这条黑线有更大的距离，这个距离叫做间距 (margin)。

当画出这两条额外的蓝线，我们看到黑色的决策界和训练样本之间有更大的最短距离。
然而粉线和蓝线离训练样本就非常近，在分离样本的时候就会比黑线表现差。因此，这个距
离叫做支持向量机的间距，而这是支持向量机具有鲁棒性的原因，因为它努力用一个最大间
距来分离样本。因此支持向量机有时被称为大间距分类器，而这其实是求解上一页幻灯片上
优化问题的结果。
我知道你也许想知道求解上一页幻灯片中的优化问题为什么会产生这个结果？它是如
何产生这个大间距分类器的呢？我知道我还没有解释这一点。
我将会从直观上略述为什么这个优化问题会产生大间距分类器。总之这个图示有助于你
理解支持向量机模型的做法，即努力将正样本和负样本用最大的间距分开。

在本节课中关于大间距分类器，我想讲最后一点：我们将这个大间距分类器中的正则化
因子常数 C 设置的非常大，我记得我将其设置为了 100000，因此对这样的一个数据集，也
许我们将选择这样的决策界，从而最大间距地分离开正样本和负样本。那么在让代价函数最
小化的过程中，我们希望找出在 y=1 和 y=0 两种情况下都使得代价函数中左边的这一项尽
量为零的参数。如果我们找到了这样的参数，则我们的最小化问题便转变成：


事实上，支持向量机现在要比这个大间距分类器所体现得更成熟，尤其是当你使用大间
距分类器的时候，你的学习算法会受异常点 (outlier) 的影响。比如我们加入一个额外的正
样本。


在这里，如果你加了这个样本，为了将样本用最大间距分开，也许我最终会得到一条类
似这样的决策界，对么？就是这条粉色的线，仅仅基于一个异常值，仅仅基于一个样本，就
将我的决策界从这条黑线变到这条粉线，这实在是不明智的。而如果正则化参数 C，设置的
非常大，这事实上正是支持向量机将会做的。它将决策界，从黑线变到了粉线，但是如果 C
设置的小一点，如果你将 C 设置的不要太大，则你最终会得到这条黑线，当然数据如果不
是线性可分的，如果你在这里有一些正样本或者你在这里有一些负样本，则支持向量机也会
将它们恰当分开。因此，大间距分类器的描述，仅仅是从直观上给出了正则化参数 C 非常大
的情形，同时，要提醒你 C 的作用类似于 1/λ，λ是我们之前使用过的正则化参数。这只是
C 非常大的情形，或者等价地λ非常小的情形。你最终会得到类似粉线这样的决策界，但是
实际上应用支持向量机的时候，当 C 不是非常非常大的时候，它可以忽略掉一些异常点的
影响，得到更好的决策界。甚至当你的数据不是线性可分的时候，支持向量机也可以给出好
的结果。
回顾 C=1/λ，因此：
C 较大时，相当于 λ 较小，可能会导致过拟合，高方差。
C 较小时，相当于 λ 较大，可能会导致低拟合，高偏差。
我们稍后会介绍支持向量机的偏差和方差，希望在那时候关于如何处理参数的这种平衡
会变得更加清晰。我希望，这节课给出了一些关于为什么支持向量机被看做大间距分类器的
直观理解。它用最大间距将样本区分开，尽管从技术上讲，这只有当参数 C 是非常大的时候
是真的，但是它对于理解支持向量机是有益的。
本节课中我们略去了一步，那就是我们在幻灯片中给出的优化问题。为什么会是这样的？
它是如何得出大间距分类器的？我在本节中没有讲解，在下一节课中，我将略述这些问题背
后的数学原理，来解释这个优化问题是如何得到一个大间距分类器的。


## 线性可分支持向量机

### SVM是什么 

SVM分类的原理就是找到一个超平面（假设数据是线性可分的），这个超平面满足两个要求： 
1. 所有数据点被完美地分成两类 
2. 所有数据点离超平面距离越远越好

### 若干定义 

为了量化以上要求，我们先定义一些概念： 
feature：x 
class：y=+1　or　−1 
function margin: \\(\hat\gamma=y(w^Tx+b)\\)
geometrical margin: \\(\tilde\gamma=\frac{\hat\gamma}{\|\|w\|\|}\\)

* 假设给定一个特征空间上的训练数据集
T={(x 1 ,y 1 ), (x 2 ,y 2 )...(x N ,y N )}
 其中,x i ∈R n ,y i ∈{+1,-1},i=1,2,...N。
* \\(x_i\\)为第i个实例(若n>1, x i 为向量);
* \\(y_i=-1\\)为x i 的类标记;
当\\(y_i=-1\\)时,称\\(x_i\\)为正例;
当\\(y_i=-1\\)时,称\\(x_i\\)为负例;
(\\(x_i\\),\\(y_i=-1\\)称为样本点。

### 如何满足两个条件 

(1) 要满足条件1，很简单，只需要满足
根据题设 \\(y(x)=w^T\Phi(x)+b\\)
$$
y(x_i)>0 y_i=+1
y(x_i)<0 y_i=-1
$$
\\(y_i(w^Tx_i+b)>0, i=1,2,...\\)
(2) 要满足条件2，比较复杂，我们可以这样做：

条件2等效于：让离超平面最近的点的geometrical margin越大越好。
假设最近点的geometrical margin为γ̃ ，那么自然其他点的geometrical margin都会大于γ̃ 。 
所以条件2 

$$\max \tilde\gamma　s.t.　\frac{y_i(w^Tx_i+b)}{\left\| w\right\|}=\frac{\hat\gamma_i}{\left\| w\right\|}=\tilde\gamma_i\ge\tilde\gamma, i=1,2,...$$

$$\max\frac{\hat\gamma}{\left\| w\right\|}　s.t.　y_i(w^Tx_i+b)=\hat\gamma_i\ge\hat\gamma, i=1,2,...$$

若固定\\(\hat\gamma=1\\)则条件2 

$$\max\frac{1}{\left\| w\right\|}　s.t. 　y_i(w^Tx_i+b)\ge1, i=1,2,...$$

其实条件2已经包含条件1， 
至此，得出SVM的目标函数就是条件2：

$$\max\frac{1}{\left\| w\right\|}　s.t. 　y_i(w^Tx_i+b)\ge1, i=1,2,...$$



对于训练数据集T，假设线性可分，其数据可以分为两类C1和C2
在样本空间中，超平面可用如下方程来描述：

$$w^{T}x+b=0\tag{1}$$

其中//(w=(w_1,w_2,...w_d)//)为法向量，决定了超平面的方向；b为位移项，是超平面与远点之间的距离。显然超平面可由法向量w和位移b唯一确定。

一般来说，一个点距离超平面的距离\\(\gamma\\)的大小可以表示分类预测的确信程度。在超平面\\(w^{T}x+b=0\\)确定的情况下，

$$\gamma=\frac{|w^Tx+b|}{||w||}$$

其中，\\(\|\|w\|\|\\)为w的二阶范数。\\(\sqrt {\textstyle \sum_{i=1}^n w_i^2}\\)

当点A表示某一实例\\(x^{(i)}\\)，其类标记为\\(y^{(i)}=+1\\)。点A与超平面的距离记作\\(\gamma^{(i)}\\)，那么 

$$\gamma^{(i)}=\frac{w^Tx^{(i)}+b}{||w||}$$

当点A表示某一实例\\(x^{(i)}\\)，其类标记为\\(y^{(i)}=-1\\)。点A与超平面的距离记作\\(\gamma^{(i)}\\)，那么 

$$\gamma^{(i)}=-\frac{w^Tx^{(i)}+b}{||w||}$$

一般地，点\\(x^{(i)}\\)与超平面的距离是 

$$\gamma^{(i)}=\frac{y^{(i)}(w^Tx^{(i)}+b)}{\left \|w \right \|}\tag{12}$$


定义超平面与整个数据集的几何间隔为： 

$$\gamma=\min \limits_{i=1,\cdots,N}\gamma^{(i)}	\tag{13}$$

其几何含义为离超平面最近的点到超平面的几何距离。

> 注意，当w和b同比例变化时，函数间隔也会同比例变化，但是超平面不变，这点性质对后面的推导很重要！！！ 

根据上面的铺垫，SVM模型可表示为： 

$$\begin{align*}  
\max\limits_{w,b}\quad & \gamma  \\
s.t.\quad & \gamma^{(i)}\geq \gamma,\quad i=1,\cdots,N	\\
\tag{16}
\end{align*}$$

同比例缩放(w,b)使得\\(y^{(i)}(w^Tx^{(i)}+b)=1\\)，而超平面不变。因此(17)可以进一步写成：

$$\begin{align*}  
\max\limits_{w,b}\quad & \frac{1}{\left \|w \right \|} \\
s.t.\quad & {y^{(i)}(w^Tx^{(i)}+b)}\geq 1,\quad i=1,\cdots,N	\\
\tag{18}
\end{align*}$$

可以再进一步写成： 

$$\begin{align*}  
\min\limits_{w,b}\quad & \frac{1}{2}{\left \|w \right \|}^2 \\
s.t.\quad & {y^{(i)}(w^Tx^{(i)}+b)}\geq 1,\quad i=1,\cdots,N	\\
\tag{19}
\end{align*}$$


这样SVM就转化为了一个凸二次规划问题，可以用一些软件包方便得求解。但是这种问题的直接求解比较低效，为此可以使用Lagrange乘子法得到其对偶问题，这样不仅能简化计算，还可以为后面引入kernel的概念做铺垫。

## 优化问题

拉格朗日乘子法

对偶函数

我们使用拉格朗日乘子法和KKT条件来求ww和bb，一个重要原因是使用拉格朗日乘子法后,还可以解决非线性划分问题。
拉格朗日乘子法和KKT条件可以解决下面这个问题：

1. 求一个最优化问题 f(x)
刚好对应我们的问题：min12‖w‖2min12‖w‖2
2. 如果存在不等式约束gk(x)<=0,k=1,…,qgk(x)<=0,k=1,…,q。
对应 subject to 1−yi(xiwT+b)<=0,i=1,...,nsubject to 1−yi(xiwT+b)<=0,i=1,...,n
3. F(x)必须是凸函数。这个也满足。

对(19)的每一个不等式约束引进拉格朗日乘子（Lagrange multiplier）\\(\alpha_i\geq0,i=1,\cdots,N\\),定义拉格朗日函数： 

$$L(w,b,\alpha)=\frac{1}{2}{\left \|w \right \|}^2-\sum_{i=1}^{N}\alpha_iy^{(i)}(w^Tx^{(i)}+b)+\sum_{i=1}^{N}\alpha_i 	\tag{20}$$


(19)的原问题（primer problem）是如下所示的极小极大问题：

$$\min\limits_{\alpha}\max\limits_{w,b}L(w,b,\alpha)	\tag{21}$$

(19)的对偶问题（dual problem）是如下所示的极大极小问题：

$$\max\limits_{\alpha}\min\limits_{w,b}L(w,b,\alpha)	\tag{22}$$


先求 \\(\min\limits_{w,b}L(w,b,\alpha)\\)。将\\(L(w,b,\alpha)\\)分别对w,b求导令其等于0：

$$\begin{align*}
\nabla_wL(w,b,\alpha)&=w-\sum_{i=1}^N\alpha_iy^{(i)}x^{(i)}=0\\
\nabla_bL(w,b,\alpha)&=-\sum_{i=1}^N\alpha_iy^{(i)}=0\\
\tag{23}
\end{align*}$$

可得： 

$$\min\limits_{w,b}L(w,b,\alpha)=-\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j{x^{(i)}}^Tx^{(i)}+\sum_{i=1}^{N}\alpha_i	\tag{24}$$

因此(22)可转化为： 


$$\begin{align*}  
\max\limits_{\alpha}\quad & -\frac{1}{2}\sum_{i=1}^{N}\sum_{j=1}^{N}\alpha_i\alpha_jy_iy_j{x^{(i)}}^Tx^{(i)}+\sum_{i=1}^{N}\alpha_i \\
s.t.\quad &\alpha_i\geq0,i=1,\cdots,N\\
& \sum_{i=1}^N\alpha_iy^{(i)}=0\\
\tag{25}
\end{align*}$$



求解出最优的α∗i后，就可以根据(23)进一步求出w∗。那怎么求b∗呢？ 
根据KKT条件： 

$$\begin{align*}
{y^{(i)}(w^Tx^{(i)}+b)} &\geq 1,\quad i=1,\cdots,N\\
\alpha_i &\geq 0,\quad i=1,\cdots,N\\
\alpha\Big({y^{(i)}(w^Tx^{(i)}+b)-1}\Big) &= 0,\quad i=1,\cdots,N\\
\nabla_wL(w,b,\alpha)=w-\sum_{i=1}^N\alpha_iy^{(i)}x^{(i)}&=0,\quad i=1,\cdots,N\\
\nabla_bL(w,b,\alpha)=-\sum_{i=1}^N\alpha_iy^{(i)}&=0,\quad i=1,\cdots,N\\
\tag{25}
\end{align*}$$

我们可以断定，∃α∗j>0（否则可推出w=0，违背超平面的定义）。注意到，当α∗j>0时，意味着y(i)(wTx(i)+b)−1=0，也即这时候样本点在最大间隔边界上（称为支持向量）。这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需要保留，最终模型仅与支持向量有关。 
w∗的表达式为： 

$$w^*=\sum_{i\in S}\alpha_iy^{(i)}x^{(i)}\tag{26}$$

其中S表示支持向量的下标集合。 
于是，理论上可以选取任意支持向量求解b∗。注意到对任意支持向量(x(s),y(s))，都有： 

$${y^{(s)}(w^Tx^{(s)}+b)-1}=0\tag{27}$$

通过(26)即可求出b∗。实际上可以采用一种更鲁棒的做法：使用所有支持向量求解的结果的平均值：

$$b^*=\frac{1}{\left \| S\right \|}\sum_{s\in S}({y^{(s)}-w^Tx^{(s)}})\tag{28}$$

## 核函数

解决线性不可分

## 松弛变量

如果存在异常点，导致无法找到一个合适的超平面，为了解决这个问题，我们引入松弛变量(slack variable)ξ。
修改之间的约束条件为：\\(w^{T}x^{i} + b >= 1 – \xi_i \qquad \text{for all i = 1, …, n}\\)


输入参数：

参数C，越大表明影响越严重。C应该一个大于0值。其实C也不能太小，太小了就约束\\(\alpha_i\\)了，比如200。
参数ξ，对所有样本数据起效的松弛变量，比如：0.0001。


## SMO 算法

## 多分类问题

[SVM目标函数的由来](http://blog.csdn.net/vincent2610/article/details/54910336)

[机器学习之深入理解SVM](http://blog.csdn.net/sinat_35512245/article/details/54984251)

[支持向量机通俗导论（理解SVM的三层境界）](http://blog.csdn.net/v_july_v/article/details/7624837)


[机器学习实战 - 读书笔记(06) – SVM支持向量机](https://www.cnblogs.com/steven-yang/p/5658362.html)
